---
title: "R-learner"
author: "Lyna EL KAMEL"
date: "2025-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(devtools) 
install_github("xnie/rlearner")
```


```{r}
library(haven)     # For reading Stata files
library(dplyr)
library(glmnet)
library(rlearner)
```



```{r}
# Accès aux données
data_path = 'C:/Users/lynae/OneDrive - GENES/Bureau/ENSAE/statapp/Base de donnée/Main Analysis and Paper/Analysis data' 
data_brut <- read_dta(paste0(data_path, "/bt_analysis_final.dta"))
```

```{r}
#Je charge mes fonctions 
source("my_functions.R")
```

# Problème des flag
```{r}
data_flag <- data_brut[
 
    data_brut$B_Saccess_mobile_flag== 1,
]
```

```{r}

```



# Modèles linéaires
Pour prédire e* et m* à partir des covariables X, je considère 3 types de modèles : 
1. Un modèle élémentaire, ne contenant que des régresseurs bruts
2. Un modèle flexible contenant tous les régresseurs bruts ainsi que des transformations et des interactions

  data_brut$B_Smuslim == 1 |
     data_brut$B_m_parttime_flag == 1 |
  data_brut$B_m_fulltime_flag == 1

```{r}
data <- data_brut[-which(is.na(data_brut$E2_Sgender_index2)),]
```


```{r}
#Divise ma base de données en 2 : 75 % pour entraîner le modèle et 25 % pour le test
set.seed(1234)
training <- sample(nrow(data), nrow(data) * (3 / 4), replace = FALSE)

data_train <- data[training, ]
data_test <- data[-training, ]
```


```{r}
data$B_Solder <- ifelse(data$B_Ssibsize - data$B_Ssibsize_young == 1, 1, 0)

```


television


```{r}
x_basic <- c( "B_Sgender_index2","B_Sage_cat", "B_Sgrade6", "B_Sclass_rank_high", "B_Sgirl", "B_Shh_size", 
"B_no_female_sib", "B_no_male_sib", "B_Solder_sister", "B_Solder", "B_Sradio_house",          

"B_rural","district", "B_Scaste",
  
"B_Sown_house",  "B_q10_guest_teachr", "B_fulltime_teacher", "B_pct_female_teacher",
  "B_Spart_extracurr", "B_Smonitor_sch", "B_Soften_bunk", "B_coed",

  "Cfem_lit_rate", "Cmale_lit_rate", "Cfem_lab_part"
)



```



## Low dimensional specification (basic)

### Prédiction de ê


```{r}
d_train <- data_train$B_treat
d_test <- data_test$B_treat
x_D <- x_basic
```

```{r}
# PROBLEME DE VARIABLES MANQUANTES : On garde uniquement les lignes complètes pour les variables utiles
vars_D <- c("B_treat", x_D)
data_train_clean <- data_train[complete.cases(data_train[, vars_D]), ]
data_test_clean <- data_test[complete.cases(data_test[, vars_D]), ]

# On construit les objets à partir de ces données nettoyées
d_train <- data_train_clean$B_treat
d_test <- data_test_clean$B_treat
```


```{r}
#Je crée ma matrice X (régresseurs) pour l'étape ê

formula_D <- as.formula(paste("B_treat", "~", paste(x_D, collapse = " + ")))
model_x_D_train <- model.matrix(formula_D, data_train_clean)
model_x_D_test <- model.matrix(formula_D, data_test_clean)
```



```{r}
#Je prédis ê avec un algo sparse (du fait de la stratification de la RCT) et dense

fit_elnet_D <- cv.glmnet(model_x_D_train, d_train, family = "binomial", alpha = .5)
dhat_elnet_D <- predict(fit_elnet_D, newx = model_x_D_test)

```

```{r}
#Je calcule l'erreur (autre chose que le r^2, à reprendre)
mse_elnet_D <- summary(lm((d_test - dhat_elnet_D)^2 ~ 1))$coef[1:2]
r2_elnet_D <- 1 - mse_elnet_D[1] / var(d_test)
cat("\nFlexible model R^2 (Elastic Net): ", r2_elnet_D)
```

Modèle terrible...

```{r}
#Je prédis ê avec un algo sparse 

fit_lasso_D <- cv.glmnet(model_x_D_train, d_train, family = "binomial", alpha = 1)
dhat_lasso_D <- predict(fit_lasso_D, newx = model_x_D_test)

```

```{r}
#Je calcule l'erreur 
mse_lasso_D <- summary(lm((d_test - dhat_lasso_D)^2 ~ 1))$coef[1:2]
r2_lasso_D <- 1 - mse_lasso_D[1] / var(d_test)
cat("\nFlexible model R^2 (Elastic Net): ", r2_lasso_D)
```


#Travail sur m^


```{r}
x <- c("B_coed", "B_rural", "B_Scaste_sc", "B_Scaste_st", "B_Smuslim", "B_no_female_sib", "B_m_edu_level", "B_Shouse_pukka_y", "B_Sgirl", "B_Sage_cat", selected_columns1, selected_columns2)
```

```{r}
# POUR L'INSTANT : On garde uniquement les lignes complètes pour les variables utiles
vars <- c("E2_Sgender_index2", x)
data_train_clean <- data_train[complete.cases(data_train[, vars]), ]
data_test_clean <- data_test[complete.cases(data_test[, vars]), ]

# On construit les objets à partir de ces données nettoyées
y_train <- data_train_clean$E2_Sgender_index2
y_test <- data_test_clean$E2_Sgender_index2
```

```{r}
#Je crée ma matrice X (régresseurs) pour l'étape m

formula_m <- as.formula(paste("E2_Sgender_index2", "~", paste(x, collapse = " + ")))
model_m_train <- model.matrix(formula_m, data_train_clean)
model_m_test <- model.matrix(formula_m, data_test_clean)
```

```{r}
#Je prédis ê avec un algo sparse 

fit_lasso_m <- cv.glmnet(model_m_train, d_train, family = "gaussian", alpha = 1)
yhat_lasso_m <- predict(fit_lasso_m, newx = model_m_test)

```


```{r}
#Je calcule l'erreur 
yhat_lasso_m_train<-predict(fit_lasso_m, newx = model_m_train)
mse_lasso_m_train <- summary(lm((y_train - yhat_lasso_m_train)^2 ~ 1))$coef[1:2]
r2_lasso_m_train <- 1 - mse_lasso_m[1] / var(y_train)
cat("\nFlexible model R^2 (LASSO): ", r2_lasso_m_train)
```


```{r}
#Je calcule l'erreur 
mse_lasso_m <- summary(lm((y_test - yhat_lasso_m)^2 ~ 1))$coef[1:2]
r2_lasso_m <- 1 - mse_lasso_m[1] / var(y_test)
cat("\nFlexible model R^2 (LASSO): ", r2_lasso_m)
```
```{r}
#Je prédis ê avec un algo dense 

fit_ridge_m <- cv.glmnet(model_m_train, d_train, family = "gaussian", alpha = 0)
yhat_ridge_m <- predict(fit_ridge_m, newx = model_m_test)

```

```{r}
#Je calcule l'erreur 
yhat_r_m_train<-predict(fit_ridge_m, newx = model_m_train)
mse_r_m_train <- summary(lm((y_train - yhat_r_m_train)^2 ~ 1))$coef[1:2]
r2_r_m_train <- 1 - mse_r_m_train[1] / var(y_train)
cat("\nFlexible model R^2 (r): ", r2_r_m_train)
```
```



```{r}
#Je calcule l'erreur 
mse_ridge_m <- summary(lm((y_test - yhat_ridge_m)^2 ~ 1))$coef[1:2]
r2_ridge_m <- 1 - mse_ridge_m[1] / var(y_test)
cat("\nFlexible model R^2 (RIDGE): ", r2_ridge_m)
```

```{r}
#Je prédis ê avec un algo dense + sparse 

fit_elv_m <- cv.glmnet(model_m_train, d_train, family = "gaussian", alpha = 0.5)
yhat_elv_m <- predict(fit_elv_m, newx = model_m_test)

```

```{r}
#Je calcule l'erreur 
mse_elv_m <- summary(lm((y_test - yhat_elv_m)^2 ~ 1))$coef[1:2]
r2_elv_m <- 1 - mse_elv_m[1] / var(y_test)
cat("\nFlexible model R^2 (ELASTNET): ", r2_elv_m)
```

#Travail sur t 

```{r}
x_test = data_test_clean[x]
y_tilde = y_test - yhat_lasso_m
X_tilde = (d_test-mean(d_test))* cbind(1, x_test)
```



```{r}

tau_fit_lasso <- cv.glmnet(X_tilde,y_tilde,family="gaussian", alpha=1)
tau_beta_lasso = as.vector(t(coef(tau_fit_lasso, s = "lambda.min")[-1]))

tau_hat_lasso = cbind(1,x_test) %*% tau_beta_lasso
tau_beta_lasso
```



```{r}

```


