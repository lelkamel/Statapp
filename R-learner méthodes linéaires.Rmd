---
title: "R-learner"
author: "Lyna EL KAMEL"
date: "2025-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(devtools) 
install_github("xnie/rlearner")
```


```{r}
library(haven)     # For reading Stata files
library(dplyr)
library(glmnet)
library(rlearner)
```



```{r}
# Accès aux données
data_path = 'C:/Users/lynae/OneDrive - GENES/Bureau/ENSAE/statapp/Base de donnée/Main Analysis and Paper/Analysis data' 
data_brut <- read_dta(paste0(data_path, "/bt_analysis_final.dta"))
```

```{r}
#Je charge mes fonctions 
source("my_functions.R")
```



# Modèles linéaires
Pour prédire e* et m* à partir des covariables X, je considère 3 types de modèles : 
1. Un modèle élémentaire, ne contenant que des régresseurs bruts
2. Un modèle flexible contenant tous les régresseurs bruts ainsi que des transformations et des interactions

  data_brut$B_Smuslim == 1 |
     data_brut$B_m_parttime_flag == 1 |
  data_brut$B_m_fulltime_flag == 1

```{r}
data <- data_brut[-which(is.na(data_brut$E2_Sgender_index2)),]
```

```{r}
data$B_Solder <- ifelse(data$B_Ssibsize - data$B_Ssibsize_young == 1, 1, 0)

```




## Modèle basique


```{r}
x_basic <- c( "B_Sgender_index2","B_Sage_cat", "B_Sgrade6", "B_Sclass_rank_high", "B_Sgirl", "B_Shh_size", 
"B_no_female_sib", "B_no_male_sib", "B_Solder_sister", "B_Solder", "B_Sradio_house",          

"B_rural","B_Sdistrict", "B_Scaste",
  
"B_Sown_house",  "B_q10_guest_teachr", "B_fulltime_teacher", "B_pct_female_teacher",
  "B_Spart_extracurr", "B_Smonitor_sch", "B_Soften_bunk", "B_coed",

  "Cfem_lit_rate", "Cmale_lit_rate", "Cfem_lab_part"
)

```


```{r}
colSums(is.na(data[ , x_basic]))#nombre de valeurs na dans chaque variable
na_data <- data[apply(data[ , x_basic], 1, function(row) any(is.na(row))), ]
data_clean <- data[!apply(data[ , x_basic], 1, function(row) any(is.na(row))), ]#J'enleve les individus concernés (3% de la base)
```

```{r}
#Divise ma base de données en 2 : 75 % pour entraîner le modèle et 25 % pour le test
set.seed(1234)
training <- sample(nrow(data_clean), nrow(data_clean) * (3 / 4), replace = FALSE)

data_train <- data_clean[training, ]
data_test <- data_clean[-training, ]
```






### Prédiction de ê


```{r}
d_train <- data_train$B_treat
d_test <- data_test$B_treat
x_D <- x_basic
```


```{r}
#Je crée ma matrice X (régresseurs) pour l'étape ê

formula_D <- as.formula(paste("B_treat", "~", paste(x_D, collapse = " + ")))
model_x_D_train <- model.matrix(formula_D, data_train)
model_x_D_test <- model.matrix(formula_D, data_test)
p_basique_D <- dim(model_x_D_train)[2]
p_basique_D
```



```{r}
#Je prédis ê avec un algo sparse (du fait de la stratification de la RCT) et dense

fit_elnet_D <- cv.glmnet(model_x_D_train, d_train, family = "binomial", alpha = .5)
dhat_elnet_D <- predict(fit_elnet_D, newx = model_x_D_test)

```

```{r}
#Je calcule l'erreur 

error_01 <- mean((dhat_elnet_D > 0.5) != d_test) # Erreur 0-1 (prédictions classées selon seuil 0.5)


# Log-loss (log-likelihood négative moyenne)
epsilon <- 1e-15  # pour éviter log(0)
p_hat <- pmin(pmax(dhat_elnet_D, epsilon), 1 - epsilon)  # stabiliser
log_loss <- -mean(d_test * log(p_hat) + (1 - d_test) * log(1 - p_hat))

cat("\nErreur logistique (log-loss) :", round(log_loss, 4))
cat("\nErreur 0-1 (taux de mauvaise classification) :", round(error_01, 4))
```

```{r}
#Je prédis ê avec un algo sparse 

fit_lasso_D <- cv.glmnet(model_x_D_train, d_train, family = "binomial", alpha = 1)
dhat_lasso_D <- predict(fit_lasso_D, newx = model_x_D_test)

```

```{r}
#Je calcule l'erreur 
error_02 <- mean((dhat_lasso_D> 0.5) != d_test) # Erreur 0-1 (prédictions classées selon seuil 0.5)


# Log-loss (log-likelihood négative moyenne)
epsilon <- 1e-15  # pour éviter log(0)
p_hat <- pmin(pmax(dhat_lasso_D, epsilon), 1 - epsilon)  # stabiliser
log_loss2 <- -mean(d_test * log(p_hat) + (1 - d_test) * log(1 - p_hat))

cat("\nErreur logistique (log-loss) :", round(log_loss2, 4))
cat("\nErreur 0-1 (taux de mauvaise classification) :", round(error_02, 4))

```


#Travail sur m^


```{r}
# On construit les objets à partir de ces données nettoyées
y_train <- data_train$E2_Sgender_index2
y_test <- data_test$E2_Sgender_index2
```

```{r}
#Je crée ma matrice X (régresseurs) pour l'étape m

formula_m <- as.formula(paste("E2_Sgender_index2", "~", paste(x_D, collapse = " + ")))
model_m_train <- model.matrix(formula_m, data_train)
model_m_test <- model.matrix(formula_m, data_test)

```

```{r}
# m avec un algo sparse 

fit_lasso_m <- cv.glmnet(model_m_train, y_train, family = "gaussian", alpha = 1)

```


```{r}
#Je calcule l'erreur 
yhat_lasso_m_train<-predict(fit_lasso_m, newx = model_m_train)
mse_lasso_m_train <- summary(lm((y_train - yhat_lasso_m_train)^2 ~ 1))$coef[1:2]
r2_lasso_m_train <- 1 - mse_lasso_m_train[1] / var(y_train)
cat("\nModel basique R^2 (LASSO), in-sample: ", r2_lasso_m_train)
```


```{r}
#Je calcule l'erreur
yhat_lasso_m_test<-predict(fit_lasso_m, newx = model_m_test)
mse_lasso_m_test <- summary(lm((y_test - yhat_lasso_m_test)^2 ~ 1))$coef[1:2]
r2_lasso_m_test <- 1 - mse_lasso_m_test[1] / var(y_test)
cat("\nModel basique R^2 (LASSO), out-sample: ", r2_lasso_m_test)
```


```{r}
#Je calcule le modèle m avec un algo dense 
fit_ridge_m <- cv.glmnet(model_m_train, y_train, family = "gaussian", alpha = 0)
```

```{r}
#Je calcule l'erreur in-sample
yhat_r_m_train<-predict(fit_ridge_m, newx = model_m_train)
mse_r_m_train <- summary(lm((y_train - yhat_r_m_train)^2 ~ 1))$coef[1:2]
r2_r_m_train <- 1 - mse_r_m_train[1] / var(y_train)
cat("\nModèle basique R^2 (r) in-sample: ", r2_r_m_train)
```

```{r}
#Je calcule l'erreur out-sample
yhat_r_m_test<-predict(fit_ridge_m, newx = model_m_test)
mse_r_m_test <- summary(lm((y_test - yhat_r_m_test)^2 ~ 1))$coef[1:2]
r2_r_m_test <- 1 - mse_r_m_test[1] / var(y_test)
cat("\nModèle basique R^2 (r) outn-sample: ", r2_r_m_test)
```


```{r}
#Je prédis ê avec un algo dense + sparse 
fit_elv_m <- cv.glmnet(model_m_train, y_train, family = "gaussian", alpha = 0.5)
```

```{r}
#Je calcule l'erreur in sample
yhat_elv_m_train <- predict(fit_elv_m, newx = model_m_train)
mse_elv_m_train <- summary(lm((y_train - yhat_elv_m_train)^2 ~ 1))$coef[1:2]
r2_elv_m_train <- 1 - mse_elv_m_train[1] / var(y_train)
cat("\nModèle basique R^2 (ELASTNET) in-sample: ", r2_elv_m_train)
```
```{r}
#Je calcule l'erreur out sample
yhat_elv_m_test <- predict(fit_elv_m, newx = model_m_test)
mse_elv_m_test <- summary(lm((y_test - yhat_elv_m_test)^2 ~ 1))$coef[1:2]
r2_elv_m_test <- 1 - mse_elv_m_test[1] / var(y_test)
cat("\nModèle basique R^2 (ELASTNET) out-sample: ", r2_elv_m_test)
```

## Modèle avec interaction 

```{r}
# Création des interactions bilinéaires (sans carrés)
interactions <- combn(x_basic, 2, FUN = function(x) paste(x[1], x[2], sep=":"))

# Création de la formule
x_flex <- paste(c(x_basic, interactions), collapse = " + ")
```

### Estimation de e
```{r}
formula_flex_D <- as.formula(paste("B_treat", "~", paste(x_flex, collapse = " + ")))
model_flex_D_train <- model.matrix(formula_flex_D, data_train)
model_flex_D_test <- model.matrix(formula_flex_D, data_test)
p_flex <- dim(model_flex_D_train)[2]
p_flex
```
```{r}
#J'utilise les 3 algo linéaires
fit_lasso_d_flex <- cv.glmnet(model_flex_D_train, d_train, family = "binomial", alpha = 1, nfolds = 5)
fit_ridge_d_flex <- cv.glmnet(model_flex_D_train, d_train, family = "binomial", alpha = 0, nfolds = 5)
fit_elnet_d_flex <- cv.glmnet(model_flex_D_train, d_train, family = "binomial", alpha = .5, nfolds = 5)

```


```{r}
#Calcul de l'erreur in-sample pour les trois
dhat_lasso_flex_train <- predict(fit_lasso_d_flex, newx = model_flex_D_train)
dhat_ridge_flex_train <- predict(fit_ridge_d_flex, newx = model_flex_D_train)
dhat_elnet_flex_train <- predict(fit_elnet_d_flex, newx = model_flex_D_train)

error_lasso_flex_train <- mean((dhat_lasso_flex_train > 0.5) != d_train) # Erreur 0-1 
error_ridge_flex_train <- mean((dhat_ridge_flex_train > 0.5) != d_train) # Erreur 0-1
error_elnet_flex_train <- mean((dhat_elnet_flex_train > 0.5) != d_train) # Erreur 0-1

# Résultat in sample (flexible model)
cat("Flexible model (Lasso) in sample : ", error_lasso_flex_train)
cat("\nFlexible model (Ridge) in sample: ", error_ridge_flex_train)
cat("\nFlexible model (Elastic Net)in sample : ", error_elnet_flex_train)

```


```{r}
#Calcul de l'erreur out-sample pour les trois
dhat_lasso_flex_test <- predict(fit_lasso_d_flex, newx = model_flex_D_test)
dhat_ridge_flex_test <- predict(fit_ridge_d_flex, newx = model_flex_D_test)
dhat_elnet_flex_test <- predict(fit_elnet_d_flex, newx = model_flex_D_test)

error_lasso_flex_test <- mean((dhat_lasso_flex_test > 0.5) != d_test) # Erreur 0-1 
error_ridge_flex_test <- mean((dhat_ridge_flex_test > 0.5) != d_test) # Erreur 0-1
error_elnet_flex_test <- mean((dhat_elnet_flex_test > 0.5) != d_test) # Erreur 0-1

# Résultat in sample (flexible model)
cat("Flexible model (Lasso) out sample : ", error_lasso_flex_test)
cat("\nFlexible model (Ridge) out sample: ", error_ridge_flex_test)
cat("\nFlexible model (Elastic Net)out sample : ", error_elnet_flex_test)

```

### Estimation de m

```{r}
formula_flex_m <- as.formula(paste("E2_Sgender_index2", "~", paste(x_flex, collapse = " + ")))
model_flex_m_train <- model.matrix(formula_flex_m , data_train)
model_flex_m_test <- model.matrix(formula_flex_m , data_test)
p_flex <- dim(model_flex_m_train)[2]
p_flex
```


```{r}
#J'utilise les 3 algo linéaires
fit_lasso_m_flex <- cv.glmnet(model_flex_m_train, y_train, family = "gaussian", alpha = 1, nfolds = 5)
fit_ridge_m_flex <- cv.glmnet(model_flex_m_train, y_train, family = "gaussian", alpha = 0, nfolds = 5)
fit_elnet_m_flex <- cv.glmnet(model_flex_m_train, y_train, family = "gaussian", alpha = .5, nfolds = 5)

```

```{r}
#Calcul de l'erreur in-sample pour les trois
yhat_lasso_flex_train <- predict(fit_lasso_m_flex, newx = model_flex_m_train)
yhat_ridge_flex_train <- predict(fit_ridge_m_flex, newx = model_flex_m_train)
yhat_elnet_flex_train <- predict(fit_elnet_m_flex, newx = model_flex_m_train)


mse_lasso_flex_m_train <- summary(lm((y_train - yhat_lasso_flex_train)^2 ~ 1))$coef[1:2]
mse_ridge_flex_m_train <- summary(lm((y_train - yhat_ridge_flex_train )^2 ~ 1))$coef[1:2]
mse_elnet_flex_m_train <- summary(lm((y_train - yhat_elnet_flex_train )^2 ~ 1))$coef[1:2]

r2_lasso_flex_m_train <- 1 - mse_lasso_flex_m_train [1] / var(y_train)
r2_ridge_flex_m_train <- 1 - mse_ridge_flex_m_train[1] / var(y_train)
r2_elnet_flex_m_train <- 1 - mse_elnet_flex_m_train[1] / var(y_train)


# Résultat in sample (flexible model)
cat("\nModèle flexible R^2 (LASSO) in-sample: ",r2_lasso_flex_m_train) 
cat("\nModèle flexible R^2 (RIDGE) in-sample: ", r2_ridge_flex_m_train) 
cat("\nModèle flexible R^2 (ELASTNET) in-sample: ", r2_elnet_flex_m_train) 

```
```{r}
#Calcul de l'erreur out-sample pour les trois
yhat_lasso_flex_train <- predict(fit_lasso_m_flex, newx = model_flex_m_train)
yhat_ridge_flex_train <- predict(fit_ridge_m_flex, newx = model_flex_m_train)
yhat_elnet_flex_train <- predict(fit_elnet_m_flex, newx = model_flex_m_train)


mse_lasso_flex_m_train <- summary(lm((y_train - yhat_lasso_flex_train)^2 ~ 1))$coef[1:2]
mse_ridge_flex_m_train <- summary(lm((y_train - yhat_ridge_flex_train )^2 ~ 1))$coef[1:2]
mse_elnet_flex_m_train <- summary(lm((y_train - yhat_elnet_flex_train )^2 ~ 1))$coef[1:2]

r2_lasso_flex_m_train <- 1 - mse_lasso_flex_m_train [1] / var(y_train)
r2_ridge_flex_m_train <- 1 - mse_ridge_flex_m_train[1] / var(y_train)
r2_elnet_flex_m_train <- 1 - mse_elnet_flex_m_train[1] / var(y_train)


# Résultat in sample (flexible model)
cat("\nModèle flexible R^2 (LASSO) in-sample: ",r2_lasso_flex_m_train) 
cat("\nModèle flexible R^2 (RIDGE) in-sample: ", r2_ridge_flex_m_train) 
cat("\nModèle flexible R^2 (ELASTNET) in-sample: ", r2_elnet_flex_m_train) 

```




#Travail sur t 

```{r}
x_test = data_test_clean[x]
y_tilde = y_test - yhat_lasso_m
X_tilde = (d_test-mean(d_test))* cbind(1, x_test)
```



```{r}
tau_fit_lasso <- cv.glmnet(X_tilde,y_tilde,family="gaussian", alpha=1)
tau_beta_lasso = as.vector(t(coef(tau_fit_lasso, s = "lambda.min")[-1]))

tau_hat_lasso = cbind(1,x_test) %*% tau_beta_lasso
tau_beta_lasso
```



